<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Back Home AR Experience</title>

  <script src="https://cdn.jsdelivr.net/gh/aframevr/aframe@1.6.0/dist/aframe-master.min.js"></script>
  <script src="https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js"></script>

  <style>
    body { margin:0; overflow:hidden; font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,sans-serif; }
    .arjs-loader { position:absolute; inset:0; display:flex; align-items:center; justify-content:center;
      background:rgba(0,0,0,.8); color:#fff; z-index:9999; font-size:1.1em; }
    .tap-overlay { position:absolute; inset:0; display:none; z-index:10000; background:rgba(0,0,0,.55);
      color:#fff; font-weight:700; align-items:center; justify-content:center; text-align:center; padding:24px; }
    .tap-btn { margin-top:12px; background:#4caf50; color:#fff; border:0; border-radius:20px; padding:10px 18px; font-weight:700; }
    .sound-chip { position:absolute; left:50%; bottom:16px; transform:translateX(-50%); background:rgba(0,0,0,.7);
      color:#fff; padding:8px 12px; border-radius:999px; font-size:14px; z-index:10001; display:none; }
  </style>

  <script>
    const isIOS = /iPad|iPhone|iPod/.test(navigator.userAgent);
    const isAndroid = /Android/i.test(navigator.userAgent);

    // --- Fit plane to marker's real size (meters), using video aspect ---
    AFRAME.registerComponent('fit-video-plane', {
      schema: {
        target: { type: 'selector' },   // #videoColor
        markerWidth: { default: 0.18 }, // meters -> set this to your print width!
        fitBy: { default: 'width' },    // 'width' | 'height'
        padding: { default: 0.0 }       // meters trimmed on each side
      },
      init() {
        const plane = this.el;
        const vid = this.data.target;
        const apply = () => {
          const vw = vid.videoWidth || 16, vh = vid.videoHeight || 9, asp = vw / vh;
          let width, height;
          if (this.data.fitBy === 'height') {
            height = this.data.markerWidth - (this.data.padding * 2);
            width  = height * asp;
          } else {
            width  = this.data.markerWidth - (this.data.padding * 2);
            height = width / asp;
          }
          plane.setAttribute('geometry', `primitive: plane; width: ${width}; height: ${height}`);
        };
        if (vid) {
          vid.addEventListener('loadedmetadata', apply);
          if (vid.readyState >= 1) apply();
        }
      }
    });

    // --- Content pose smoothing (position + rotation) ---
    AFRAME.registerComponent('pose-smoother', {
      schema: { pos: {default: 0.18}, rot: {default: 0.18} },
      init() {
        const o = this.el.object3D;
        this._p = o.position.clone();
        this._q = o.quaternion.clone();
      },
      tick() {
        const o = this.el.object3D;
        this._p.lerp(o.position, this.data.pos); o.position.copy(this._p);
        this._q.slerp(o.quaternion, this.data.rot); o.quaternion.copy(this._q);
      }
    });

    // --- Videoplayer: arm audio on tap; start audio on markerFound; grace on lost ---
    AFRAME.registerComponent('videoplayer', {
      schema: { confirmMs: {default: 0}, lostMs: {default: 900} },
      init() {
        const videoColor = document.querySelector('#videoColor');
        const videoAlpha = document.querySelector('#videoAlpha');
        const tapOverlay = document.getElementById('tapOverlay');
        const tapBtn = document.getElementById('tapBtn');
        const soundChip = document.getElementById('soundChip');

        // Start muted for autoplay policies
        [videoColor, videoAlpha].forEach(v => { v.muted = true; v.volume = 0; v.setAttribute('muted',''); });

        let armed = false;   // set after user gesture
        let live  = false;   // true while audio is unmuted
        let ct = null;       // confirm timer
        let lt = null;       // lost grace timer

        // UI for arming audio
        if (isIOS) tapOverlay.style.display = 'flex';
        else if (isAndroid) soundChip.style.display = 'block';

        const arm = async () => {
          try {
            // Prime decoding while muted (no sound yet)
            await Promise.allSettled([ videoColor.play(), videoAlpha.play() ]);
            videoColor.muted = true; videoColor.volume = 0;
            armed = true;
            tapOverlay.style.display = 'none';
            soundChip.style.display = 'none';
          } catch (e) {
            // keep UI visible to try again
          }
        };

        tapBtn.addEventListener('click', arm, { once: true });
        tapOverlay.addEventListener('click', arm, { once: true });
        soundChip.addEventListener('click', arm, { once: true });

        // Marker events (START sound on detection, STOP after lost grace)
        this.el.addEventListener('markerFound', () => {
          // Cancel any pending lost
          if (lt) { clearTimeout(lt); lt = null; }

          // Make sure frames are running
          if (videoColor.paused) videoColor.play().catch(()=>{});
          if (videoAlpha.paused) videoAlpha.play().catch(()=>{});

          const startAudio = async () => {
            if (armed && !live) {
              try { videoColor.muted = false; videoColor.volume = 1; await videoColor.play(); live = true; }
              catch (e) { live = false; tapOverlay.style.display = 'flex'; }
            }
          };

          if (ct) clearTimeout(ct);
          if (this.data.confirmMs <= 0) startAudio();
          else ct = setTimeout(startAudio, this.data.confirmMs);
        });

        this.el.addEventListener('markerLost', () => {
          if (ct) { clearTimeout(ct); ct = null; }
          if (lt) clearTimeout(lt);
          lt = setTimeout(() => {
            live = false;
            videoColor.muted = true; videoColor.volume = 0;
            if (!videoColor.paused) videoColor.pause();
            if (!videoAlpha.paused) videoAlpha.pause();
          }, this.data.lostMs);
        });
      }
    });
  </script>
</head>

<body style="margin:0; overflow:hidden;">
  <div class="arjs-loader" id="loader"><div>Loading, please waitâ€¦</div></div>

  <!-- One clear tap to ARM audio (no sound yet). Sound begins on detection. -->
  <div class="tap-overlay" id="tapOverlay">
    <div>
      <div>Tap once to arm audio. Sound starts when the image is detected.</div>
      <button class="tap-btn" id="tapBtn">Arm Audio</button>
    </div>
  </div>
  <div class="sound-chip" id="soundChip">ðŸ”Š Tap to enable sound</div>

  <a-scene
    vr-mode-ui="enabled:false"
    renderer="antialias:true; logarithmicDepthBuffer:true; precision:high;"
    embedded
    arjs="sourceType: webcam; trackingMethod: best; debugUIEnabled: false;
          maxDetectionRate: 60; canvasWidth: 1280; canvasHeight: 960;"
    onloaded="document.getElementById('loader').style.display='none';"
  >
    <a-assets timeout="3000">
      <video id="videoColor"
             src="https://raw.githack.com/alevalve/AR_Crab_Backhome/main/AR_Video02_Color.mp4"
             preload="metadata" loop muted crossorigin="anonymous" webkit-playsinline playsinline></video>
      <video id="videoAlpha"
             src="https://raw.githack.com/alevalve/AR_Crab_Backhome/main/AR_Video02_Alpha.mp4"
             preload="metadata" loop muted crossorigin="anonymous" webkit-playsinline playsinline></video>
    </a-assets>

    <!-- Transparency matte shader -->
    <script id="matte-shader" type="x-shader/x-fragment">
      precision mediump float;
      uniform sampler2D videoColor; uniform sampler2D videoAlpha; varying vec2 vUV;
      void main(){ vec4 c=texture2D(videoColor,vUV); vec4 a=texture2D(videoAlpha,vUV); gl_FragColor=vec4(c.rgb,a.r); }
    </script>
    <script>
      AFRAME.registerShader('video-matte-shader', {
        schema:{ videoColor:{type:'map', is:'uniform'}, videoAlpha:{type:'map', is:'uniform'} },
        fragmentShader:document.getElementById('matte-shader').textContent,
        vertexShader:'varying vec2 vUV; void main(){ vUV=uv; gl_Position=projectionMatrix*modelViewMatrix*vec4(position,1.0); }'
      });
    </script>

    <!-- IMPORTANT: url must point to the NFT dataset ROOT (no extension) -->
    <a-nft
      videoplayer="confirmMs: 0; lostMs: 900"
      type="nft"
      url="https://raw.githack.com/alevalve/AR_Crab_Backhome/main/marker/AR_Image-tracker_Crab"
      smooth="true"
      smoothCount="30"
      smoothTolerance="0.01"
      smoothThreshold="10"
    >
      <!-- Set markerWidth to your printed image's real width (meters). Example: 0.18 = 18 cm -->
      <a-entity id="videoPlane"
        pose-smoother="pos:0.18; rot:0.18"
        fit-video-plane="target:#videoColor; markerWidth:0.18; fitBy:width; padding:0.003"
        geometry="primitive: plane; width: 0.18; height: 0.1"  <!-- will be overridden by fit-video-plane -->
        material="shader: video-matte-shader; videoColor:#videoColor; videoAlpha:#videoAlpha; transparent:true; alphaTest:0.1;"
        position="0 0 0.005" rotation="-90 0 0">
      </a-entity>
    </a-nft>

    <a-entity camera look-controls position="0 1.6 0"></a-entity>
  </a-scene>
</body>
</html>
