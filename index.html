<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Stable Back Home AR Experience</title>

  <script src="https://cdn.jsdelivr.net/gh/aframevr/aframe@1.6.0/dist/aframe-master.min.js"></script>
  <script src="https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js"></script>

  <style>
    body { margin: 0; overflow: hidden; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; }
    .arjs-loader {
      position: absolute; inset: 0; display: flex; align-items: center; justify-content: center;
      background: rgba(0,0,0,0.85); color: #fff; z-index: 9999;
    }
    .gate {
      position: absolute; inset: 0; display: flex; align-items: center; justify-content: center;
      z-index: 10000; background: rgba(0,0,0,0.55); color: #fff; text-align: center; padding: 24px;
    }
    .gate[hidden] { display: none; }
    .btn {
      margin-top: 12px; background: #4caf50; color: #fff; border: 0; border-radius: 20px; padding: 10px 18px; font-weight: 700; cursor: pointer;
    }
    .debug { position: absolute; top: 8px; left: 8px; z-index: 10001; background: rgba(0,0,0,0.75); color:#fff; padding:6px 8px; border-radius:6px; font-size:12px; }
  </style>

  <script>
    const isIOS = /iPad|iPhone|iPod/.test(navigator.userAgent);
    const isAndroid = /Android/i.test(navigator.userAgent);

    // Enhanced video plane fitting
    AFRAME.registerComponent('fit-video-plane', {
      schema: { target: {type: 'selector'}, baseWidth: {default: 300} },
      init() {
        const plane = this.el;
        const video = this.data.target;
        if (!video) return;
        
        const setSize = () => {
          const w = video.videoWidth || 16, h = video.videoHeight || 9;
          const aspect = w / h;
          plane.setAttribute('geometry', `primitive: plane; width: ${this.data.baseWidth}; height: ${this.data.baseWidth / aspect}`);
        };
        
        video.addEventListener('loadedmetadata', setSize);
        if (video.readyState >= 1) setSize();
      }
    });

    // Enhanced stabilization with position, rotation, and scale smoothing
    AFRAME.registerComponent('stabilize', {
      schema: { 
        positionFactor: {default: 0.1},
        rotationFactor: {default: 0.2},
        scaleFactor: {default: 0.3}
      },
      
      init() {
        this.prevPosition = new THREE.Vector3();
        this.prevRotation = new THREE.Euler();
        this.prevScale = new THREE.Vector3(1, 1, 1);
        
        // Store initial values
        const obj = this.el.object3D;
        this.prevPosition.copy(obj.position);
        this.prevRotation.copy(obj.rotation);
        this.prevScale.copy(obj.scale);
      },
      
      tick() {
        const obj = this.el.object3D;
        
        // Smooth position
        this.prevPosition.lerp(obj.position, this.data.positionFactor);
        obj.position.copy(this.prevPosition);
        
        // Smooth rotation using slerp (spherical interpolation)
        const targetQuat = new THREE.Quaternion().setFromEuler(obj.rotation);
        const currentQuat = new THREE.Quaternion().setFromEuler(this.prevRotation);
        currentQuat.slerp(targetQuat, this.data.rotationFactor);
        obj.rotation.setFromQuaternion(currentQuat);
        this.prevRotation.copy(obj.rotation);
        
        // Smooth scale
        this.prevScale.lerp(obj.scale, this.data.scaleFactor);
        obj.scale.copy(this.prevScale);
      }
    });

    // Play audio exactly on marker detection
    AFRAME.registerComponent('videoplayer', {
      init: function () {
        const debug = (msg) => { const d = document.getElementById('debug'); if (d) d.textContent = msg; };

        const videoColor = document.querySelector('#videoColor');
        const videoAlpha = document.querySelector('#videoAlpha');
        const gate = document.getElementById('gate');
        const btn = document.getElementById('enableBtn');

        // Start fully muted to satisfy autoplay rules
        videoColor.setAttribute('muted', ''); videoColor.muted = true; videoColor.volume = 0;
        videoAlpha.setAttribute('muted', ''); videoAlpha.muted = true; videoAlpha.volume = 0;

        let audioArmed = false;   // set to true after the ONE user gesture
        let audioLive  = false;   // true while marker is found & color is unmuted

        // One explicit gesture to "arm" audio (no sound yet)
        const armAudio = async () => {
          try {
            // Prime decoding so unmute on detection is instant
            await Promise.allSettled([ videoColor.play(), videoAlpha.play() ]);
            // Keep muted until the marker is detected
            videoColor.muted = true;  videoColor.volume = 0;
            audioArmed = true;
            gate.hidden = true;
            debug('Audio armed. Waiting for marker…');
          } catch (e) {
            debug('Tap again to arm audio');
            console.warn('Arming error:', e);
          }
        };
        btn.addEventListener('click', armAudio, { once: true });

        // Marker events
        this.el.addEventListener('markerFound', async () => {
          debug('Marker found');
          // Make sure frames are flowing
          if (videoColor.paused) videoColor.play().catch(()=>{});
          if (videoAlpha.paused) videoAlpha.play().catch(()=>{});

          // If armed, start audio NOW (unmute exactly on detection)
          if (audioArmed && !audioLive) {
            try {
              videoColor.muted = false; videoColor.volume = 1.0;
              await videoColor.play();
              audioLive = true;
              debug('Marker found — audio started');
            } catch (e) {
              // If browser still refuses, leave gate visible to try again
              audioLive = false;
              gate.hidden = false;
              debug('Tap to enable audio');
              console.warn('Unmute-on-detect error:', e);
            }
          }
        });

        this.el.addEventListener('markerLost', () => {
          debug('Marker lost');
          // Stop everything and re-mute so the next detection starts fresh
          audioLive = false;
          videoColor.muted = true; videoColor.volume = 0;
          if (!videoColor.paused) videoColor.pause();
          if (!videoAlpha.paused) videoAlpha.pause();
        });

        // Show the gate for one clean gesture on any platform
        gate.hidden = false;
      }
    });
  </script>
</head>

<body style="margin: 0; overflow: hidden;">
  <div class="arjs-loader" id="loader"><div>Loading, please wait…</div></div>

  <!-- One clear gesture to "arm" audio (no sound yet). Sound begins on marker detection. -->
  <div class="gate" id="gate" hidden>
    <div>
      <div>Tap once to arm audio. Sound will start when the image is detected.</div>
      <button class="btn" id="enableBtn">Arm Audio</button>
    </div>
  </div>
  <div class="debug" id="debug">Init…</div>

  <a-scene
    vr-mode-ui="enabled: false;"
    renderer="logarithmicDepthBuffer: true; precision: high; antialias: true;"
    embedded
    arjs="sourceType: webcam; trackingMethod: best; debugUIEnabled: false; 
          sourceWidth: 1280; sourceHeight: 720; displayWidth: 1280; displayHeight: 720;"
    onloaded="document.getElementById('loader').style.display='none';"
  >
    <a-assets>
      <!-- Keep muted; we unmute the color track exactly on markerFound -->
      <video
        id="videoColor"
        src="https://raw.githack.com/alevalve/AR_Crab_Backhome/main/AR_Video02_Color.mp4"
        preload="auto"
        loop
        muted
        crossorigin="anonymous"
        webkit-playsinline
        playsinline
      ></video>

      <video
        id="videoAlpha"
        src="https://raw.githack.com/alevalve/AR_Crab_Backhome/main/AR_Video02_Alpha.mp4"
        preload="auto"
        loop
        muted
        crossorigin="anonymous"
        webkit-playsinline
        playsinline
      ></video>
    </a-assets>

    <!-- Transparency matte shader -->
    <script id="matte-shader" type="x-shader/x-fragment">
      precision mediump float;
      uniform sampler2D videoColor;
      uniform sampler2D videoAlpha;
      varying vec2 vUV;
      void main() {
        vec4 color = texture2D(videoColor, vUV);
        vec4 alpha = texture2D(videoAlpha, vUV);
        gl_FragColor = vec4(color.rgb, alpha.r);
      }
    </script>

    <script>
      AFRAME.registerShader('video-matte-shader', {
        schema: {
          videoColor: { type: 'map', is: 'uniform' },
          videoAlpha: { type: 'map', is: 'uniform' }
        },
        fragmentShader: document.getElementById('matte-shader').textContent,
        vertexShader: `
          varying vec2 vUV;
          void main() {
            vUV = uv;
            gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);
          }
        `
      });
    </script>

    <!-- NFT marker with enhanced smoothing -->
    <a-nft
      videoplayer
      type="nft"
      url="https://raw.githack.com/alevalve/AR_Crab_Backhome/main/marker/AR_Image-tracker_Crab"
      smooth="true"
      smoothCount="20"
      smoothTolerance="0.001"
      smoothThreshold="3"
      emitevents="true"
    >
      <!-- Plane: auto-fit to color video aspect; enhanced stabilizer -->
      <a-entity id="videoPlane"
        fit-video-plane="target: #videoColor; baseWidth: 300"
        stabilize="positionFactor: 0.08; rotationFactor: 0.15; scaleFactor: 0.2"
        geometry="primitive: plane; width: 300; height: 150"
        material="shader: video-matte-shader;
                  videoColor: #videoColor;
                  videoAlpha: #videoAlpha;
                  transparent: true;"
        position="0 0 0.1"
        rotation="-90 0 0"
      ></a-entity>
    </a-nft>

    <a-entity camera look-controls position="0 1.6 0"></a-entity>
  </a-scene>
</body>
</html>