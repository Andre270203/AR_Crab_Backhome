<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Back Home AR Experience - Adaptive Scaling</title>

  <script src="https://cdn.jsdelivr.net/gh/aframevr/aframe@1.6.0/dist/aframe-master.min.js"></script>
  <script src="https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js"></script>

  <style>
    body { margin: 0; overflow: hidden; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; }
    .arjs-loader {
      position: absolute; inset: 0; display: flex; align-items: center; justify-content: center;
      background: rgba(0,0,0,0.85); color: #fff; z-index: 9999;
    }
    .gate {
      position: absolute; inset: 0; display: flex; align-items: center; justify-content: center;
      z-index: 10000; background: rgba(0,0,0,0.55); color: #fff; text-align: center; padding: 24px;
    }
    .gate[hidden] { display: none; }
    .btn {
      margin-top: 12px; background: #4caf50; color: #fff; border: 0; border-radius: 20px; padding: 10px 18px; font-weight: 700; cursor: pointer;
    }
    .debug { position: absolute; top: 8px; left: 8px; z-index: 10001; background: rgba(0,0,0,0.75); color:#fff; padding:6px 8px; border-radius:6px; font-size:12px; }
  </style>

  <script>
    const isIOS = /iPad|iPhone|iPod/.test(navigator.userAgent);
    const isAndroid = /Android/i.test(navigator.userAgent);

    // Auto-fit the plane to the color video aspect ratio
    AFRAME.registerComponent('fit-video-plane', {
      schema: { target: {type: 'selector'}, baseWidth: {default: 300} },
      init() {
        const plane = this.el;
        const video = this.data.target;
        if (!video) return;
        const setSize = () => {
          const w = video.videoWidth || 16, h = video.videoHeight || 9;
          const aspect = w / h;
          plane.setAttribute('geometry', `primitive: plane; width: ${this.data.baseWidth}; height: ${this.data.baseWidth / aspect}`);
        };
        video.addEventListener('loadedmetadata', setSize);
        if (video.readyState >= 1) setSize();
      }
    });

    // Distance-based adaptive scaling component
    AFRAME.registerComponent('adaptive-scale', {
      schema: {
        baseSize: {default: 300},        // Base size at reference distance
        referenceDistance: {default: 50}, // Reference distance in cm
        minScale: {default: 0.3},        // Minimum scale factor
        maxScale: {default: 2.5},        // Maximum scale factor
        smoothing: {default: 0.15}       // Smoothing factor for scale changes
      },
      init() {
        this.camera = document.querySelector('[camera]');
        this.marker = this.el.parentNode; // The NFT marker
        this.currentScale = 1;
        this.targetScale = 1;
        this.baseGeometry = null;
      },
      tick() {
        if (!this.camera || !this.marker) return;
        
        // Get distance between camera and marker
        const cameraPos = this.camera.object3D.position;
        const markerPos = this.marker.object3D.position;
        const distance = cameraPos.distanceTo(markerPos);
        
        // Convert to centimeters (A-Frame uses meters)
        const distanceCm = distance * 100;
        
        // Calculate scale based on distance
        const scaleRatio = this.data.referenceDistance / distanceCm;
        this.targetScale = Math.max(this.data.minScale, Math.min(this.data.maxScale, scaleRatio));
        
        // Smooth the scale transition
        this.currentScale += (this.targetScale - this.currentScale) * this.data.smoothing;
        
        // Apply scale to the element
        const newSize = this.data.baseSize * this.currentScale;
        
        // Update geometry while maintaining aspect ratio
        const geometry = this.el.getAttribute('geometry');
        if (geometry) {
          const aspectRatio = geometry.height / geometry.width;
          this.el.setAttribute('geometry', {
            width: newSize,
            height: newSize * aspectRatio
          });
        }
        
        // Debug info
        const debug = document.getElementById('debug');
        if (debug) {
          debug.textContent = `Distance: ${distanceCm.toFixed(0)}cm | Scale: ${this.currentScale.toFixed(2)}x`;
        }
      }
    });

    // Enhanced stabilizer with better smoothing
    AFRAME.registerComponent('stabilize', {
      schema: { 
        positionFactor: {default: 0.2},  // More responsive
        rotationFactor: {default: 0.15}, // More responsive
        scaleStabilize: {default: true}   // Whether to stabilize scaling
      },
      init() {
        this.prevPosition = null;
        this.prevRotation = null;
        this.prevScale = null;
      },
      tick() {
        const obj = this.el.object3D;
        
        // Smooth position
        if (!this.prevPosition) this.prevPosition = obj.position.clone();
        this.prevPosition.lerp(obj.position, this.data.positionFactor);
        obj.position.copy(this.prevPosition);
        
        // Smooth rotation
        if (!this.prevRotation) this.prevRotation = obj.quaternion.clone();
        this.prevRotation.slerp(obj.quaternion, this.data.rotationFactor);
        obj.quaternion.copy(this.prevRotation);
        
        // Optional scale stabilization (lighter for adaptive scaling)
        if (this.data.scaleStabilize) {
          if (!this.prevScale) this.prevScale = obj.scale.clone();
          this.prevScale.lerp(obj.scale, 0.3); // Lighter scale smoothing
          obj.scale.copy(this.prevScale);
        }
      }
    });

    // Improved persistence component
    AFRAME.registerComponent('persistent', {
      schema: {
        timeout: {default: 800}, // ms to keep visible after marker lost
        fadeOut: {default: false} // Disable fade for better performance
      },
      init() {
        this.timeout = null;
      },
      markerFound() {
        this.el.object3D.visible = true;
        if (this.timeout) {
          clearTimeout(this.timeout);
          this.timeout = null;
        }
      },
      markerLost() {
        if (this.data.timeout > 0) {
          this.timeout = setTimeout(() => {
            this.el.object3D.visible = false;
          }, this.data.timeout);
        } else {
          this.el.object3D.visible = false;
        }
      }
    });

    // Enhanced angle-based visibility
    AFRAME.registerComponent('angle-visibility', {
      schema: {
        maxAngle: {default: 60} // Maximum viewing angle in degrees
      },
      init() {
        this.camera = document.querySelector('[camera]');
        this.marker = this.el.parentNode;
      },
      tick() {
        if (!this.camera || !this.marker) return;
        
        // Calculate angle between camera and marker normal
        const cameraDir = new THREE.Vector3();
        this.camera.object3D.getWorldDirection(cameraDir);
        
        const markerNormal = new THREE.Vector3(0, 0, 1);
        this.marker.object3D.localToWorld(markerNormal);
        markerNormal.sub(this.marker.object3D.position).normalize();
        
        const angle = Math.acos(cameraDir.dot(markerNormal.negate())) * 180 / Math.PI;
        
        // Show/hide based on angle
        const visible = angle < this.data.maxAngle;
        this.el.object3D.visible = visible;
      }
    });

    // Play audio exactly on marker detection
    AFRAME.registerComponent('videoplayer', {
      init: function () {
        const debug = (msg) => { 
          const d = document.getElementById('debug'); 
          if (d && !d.textContent.includes('Distance:')) d.textContent = msg; 
        };

        const videoColor = document.querySelector('#videoColor');
        const videoAlpha = document.querySelector('#videoAlpha');
        const gate = document.getElementById('gate');
        const btn = document.getElementById('enableBtn');

        // Start fully muted to satisfy autoplay rules
        videoColor.setAttribute('muted', ''); videoColor.muted = true; videoColor.volume = 0;
        videoAlpha.setAttribute('muted', ''); videoAlpha.muted = true; videoAlpha.volume = 0;

        let audioArmed = false;
        let audioLive = false;

        // One explicit gesture to "arm" audio
        const armAudio = async () => {
          try {
            await Promise.allSettled([ videoColor.play(), videoAlpha.play() ]);
            videoColor.muted = true; videoColor.volume = 0;
            audioArmed = true;
            gate.hidden = true;
            debug('Audio armed. Waiting for marker…');
          } catch (e) {
            debug('Tap again to arm audio');
            console.warn('Arming error:', e);
          }
        };
        btn.addEventListener('click', armAudio, { once: true });

        // Marker events
        this.el.addEventListener('markerFound', async () => {
          debug('Marker found');
          
          // Notify persistence component
          const videoPlane = document.getElementById('videoPlane');
          const persistent = videoPlane.components.persistent;
          if (persistent) persistent.markerFound();
          
          // Make sure frames are flowing
          if (videoColor.paused) videoColor.play().catch(()=>{});
          if (videoAlpha.paused) videoAlpha.play().catch(()=>{});

          // Start audio if armed
          if (audioArmed && !audioLive) {
            try {
              videoColor.muted = false; videoColor.volume = 1.0;
              await videoColor.play();
              audioLive = true;
              debug('Marker found — audio started');
            } catch (e) {
              audioLive = false;
              gate.hidden = false;
              debug('Tap to enable audio');
              console.warn('Unmute-on-detect error:', e);
            }
          }
        });

        this.el.addEventListener('markerLost', () => {
          debug('Marker lost');
          
          // Notify persistence component
          const videoPlane = document.getElementById('videoPlane');
          const persistent = videoPlane.components.persistent;
          if (persistent) persistent.markerLost();
          
          // Keep videos playing briefly for quick re-detection
          setTimeout(() => {
            audioLive = false;
            videoColor.muted = true; videoColor.volume = 0;
            if (!videoColor.paused) videoColor.pause();
            if (!videoAlpha.paused) videoAlpha.pause();
          }, 300);
        });

        gate.hidden = false;
      }
    });
  </script>
</head>

<body style="margin: 0; overflow: hidden;">
  <div class="arjs-loader" id="loader"><div>Loading, please wait…</div></div>

  <div class="gate" id="gate" hidden>
    <div>
      <div>Tap once to arm audio. Sound will start when the image is detected.</div>
      <button class="btn" id="enableBtn">Arm Audio</button>
    </div>
  </div>
  <div class="debug" id="debug">Init…</div>

  <a-scene
    vr-mode-ui="enabled: false;"
    renderer="logarithmicDepthBuffer: true; precision: medium;"
    embedded
    arjs="sourceType: webcam; trackingMethod: best; debugUIEnabled: false;"
    onloaded="document.getElementById('loader').style.display='none';"
  >
    <a-assets>
      <video
        id="videoColor"
        src="https://raw.githack.com/alevalve/AR_Crab_Backhome/main/AR_Video02_Color.mp4"
        preload="metadata"
        loop
        muted
        crossorigin="anonymous"
        webkit-playsinline
        playsinline
      ></video>

      <video
        id="videoAlpha"
        src="https://raw.githack.com/alevalve/AR_Crab_Backhome/main/AR_Video02_Alpha.mp4"
        preload="metadata"
        loop
        muted
        crossorigin="anonymous"
        webkit-playsinline
        playsinline
      ></video>
    </a-assets>

    <!-- Transparency matte shader -->
    <script id="matte-shader" type="x-shader/x-fragment">
      precision mediump float;
      uniform sampler2D videoColor;
      uniform sampler2D videoAlpha;
      varying vec2 vUV;
      void main() {
        vec4 color = texture2D(videoColor, vUV);
        vec4 alpha = texture2D(videoAlpha, vUV);
        gl_FragColor = vec4(color.rgb, alpha.r);
      }
    </script>

    <script>
      AFRAME.registerShader('video-matte-shader', {
        schema: {
          videoColor: { type: 'map', is: 'uniform' },
          videoAlpha: { type: 'map', is: 'uniform' }
        },
        fragmentShader: document.getElementById('matte-shader').textContent,
        vertexShader: `
          varying vec2 vUV;
          void main() {
            vUV = uv;
            gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);
          }
        `
      });
    </script>

    <!-- NFT marker with optimized settings -->
    <a-nft
      videoplayer
      type="nft"
      url="https://raw.githack.com/alevalve/AR_Crab_Backhome/main/marker/AR_Image-tracker_Crab"
      smooth="true"
      smoothCount="10"
      smoothTolerance="0.01"
      smoothThreshold="5"
    >
      <!-- Video plane with adaptive scaling and stabilization -->
      <a-entity id="videoPlane"
        fit-video-plane="target: #videoColor; baseWidth: 300"
        adaptive-scale="baseSize: 300; referenceDistance: 50; minScale: 0.4; maxScale: 2.0; smoothing: 0.12"
        stabilize="positionFactor: 0.25; rotationFactor: 0.18; scaleStabilize: true"
        persistent="timeout: 600; fadeOut: false"
        geometry="primitive: plane; width: 300; height: 150"
        material="shader: video-matte-shader;
                  videoColor: #videoColor;
                  videoAlpha: #videoAlpha;
                  transparent: true;"
        position="0 0 0.1"
        rotation="-90 0 0"
      ></a-entity>
    </a-nft>

    <a-entity camera look-controls position="0 1.6 0"></a-entity>
  </a-scene>
</body>
</html>