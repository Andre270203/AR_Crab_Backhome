<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Back Home AR — NFT Marker (Color+Alpha)</title>

  <!-- A-Frame -->
  <script src="https://aframe.io/releases/1.4.0/aframe.min.js"></script>
  <!-- AR.js for A-Frame (NFT / image tracking) -->
  <script src="https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover" />
  <meta http-equiv="origin-trial" content="" />

  <style>
    html, body {
      margin: 0; height: 100%; overflow: hidden;
      font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif;
      background: transparent; /* don't paint behind the canvas */
    }
    /* Ensure A-Frame canvas is transparent */
    canvas.a-canvas { background: transparent !important; }

    #loader {
      position: fixed; inset: 0;
      display: flex; align-items: center; justify-content: center;
      background: rgba(0,0,0,0.85); color: #fff; z-index: 1000;
      font-size: 18px;
    }
    #tapOverlay {
      position: fixed; inset: 0;
      display: none; align-items: center; justify-content: center; text-align: center;
      background: rgba(0,0,0,0.7); color: #fff; z-index: 1001;
    }
    #tapOverlay button {
      margin-top: 16px; padding: 12px 20px; border: 0; border-radius: 8px;
      background: #1976d2; color: #fff; font-weight: 700; font-size: 16px; cursor: pointer;
    }
    .hidden { display: none !important; }
  </style>

  <!-- Fragment shader for video + alpha compositing -->
  <script id="matte-shader" type="x-shader/x-fragment">
    precision mediump float;
    uniform sampler2D videoColor;
    uniform sampler2D videoAlpha;
    uniform float opacity;
    varying vec2 vUV;
    void main() {
      vec4 color = texture2D(videoColor, vUV);
      float alpha = texture2D(videoAlpha, vUV).r * opacity;
      gl_FragColor = vec4(color.rgb, alpha);
    }
  </script>
</head>

<body>
  <!-- Loader -->
  <div id="loader">Loading AR Experience…</div>

  <!-- Audio unlock (required on iOS/Android for unmuted playback) -->
  <div id="tapOverlay">
    <div>
      <div style="font-size:20px;margin-bottom:8px;">AR Ready</div>
      <div>Tap to enable audio & start</div>
      <button id="tapBtn">Start</button>
    </div>
  </div>

  <!-- A-Frame scene with AR.js (marker-based) -->
  <a-scene
    embedded
    vr-mode-ui="enabled: false"
    renderer="alpha: true; antialias: true; precision: medium"
    arjs="sourceType: webcam; detectionMode: mono_and_matrix; trackingMethod: best; debugUIEnabled: false; videoTexture: true;"
  >
    <a-assets>
      <!-- Your dual videos -->
      <video id="videoColor"
             src="https://raw.githack.com/alevalve/AR_Crab_Backhome/main/AR_Video02_Color.mp4"
             preload="auto" loop muted crossorigin="anonymous" playsinline webkit-playsinline>
      </video>
      <video id="videoAlpha"
             src="https://raw.githack.com/alevalve/AR_Crab_Backhome/main/AR_Video02_Alpha.mp4"
             preload="auto" loop muted crossorigin="anonymous" playsinline webkit-playsinline>
      </video>
    </a-assets>

    <!-- NFT marker root: replace URL with your NFT marker folder path (no extension) -->
    <a-nft
      type="nft"
      url="https://raw.githack.com/alevalve/AR_Crab_Backhome/main/marker/AR_Image-tracker_Crab"
      smooth="true" smoothCount="10" smoothTolerance="0.01" smoothThreshold="5"
      emitevents="true"
    >
      <!-- Plane with matte shader (starts hidden; shown on markerFound) -->
      <a-plane
        id="videoPlane"
        position="0 0 0"
        rotation="-90 0 0"
        width="1.2" height="0.67"
        visible="false"
        material="shader: video-matte; videoColor: #videoColor; videoAlpha: #videoAlpha; transparent: true; alphaTest: 0.001; opacity: 1"
      ></a-plane>
    </a-nft>

    <!-- Camera (AR.js manages the real video feed behind the 3D content) -->
    <a-entity camera></a-entity>
  </a-scene>

  <script>
    // Register the matte shader
    AFRAME.registerShader('video-matte', {
      schema: {
        videoColor: { type: 'map', is: 'uniform' },
        videoAlpha: { type: 'map', is: 'uniform' },
        opacity:    { type: 'number', default: 1.0, is: 'uniform' }
      },
      fragmentShader: document.getElementById('matte-shader').textContent,
      vertexShader: `
        varying vec2 vUV;
        void main() {
          vUV = uv;
          gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);
        }
      `
    });

    // Basic flow:
    // 1) Wait for scene load -> hide loader, show tap overlay on mobile
    // 2) On user tap -> unmute allowed, we keep videos paused until markerFound
    // 3) markerFound -> show plane, play videos (unmute if user tapped)
    // 4) markerLost  -> pause videos, hide plane

    document.addEventListener('DOMContentLoaded', () => {
      const scene      = document.querySelector('a-scene');
      const loader     = document.getElementById('loader');
      const tapOverlay = document.getElementById('tapOverlay');
      const tapBtn     = document.getElementById('tapBtn');

      const videoColor = document.getElementById('videoColor');
      const videoAlpha = document.getElementById('videoAlpha');
      const videoPlane = document.getElementById('videoPlane');
      const marker     = document.querySelector('a-nft');

      let audioUnlocked = false;

      function onSceneLoaded() {
        // Ensure transparent clear so the camera feed shows under WebGL
        if (scene.renderer) scene.renderer.setClearColor(0x000000, 0);
        loader.classList.add('hidden');

        const isMobile = /Android|iPhone|iPad|iPod/i.test(navigator.userAgent);
        if (isMobile) tapOverlay.style.display = 'flex';
      }

      if (scene.hasLoaded) onSceneLoaded();
      else scene.addEventListener('loaded', onSceneLoaded);

      function unlockAudio() {
        audioUnlocked = true;
        tapOverlay.classList.add('hidden');

        // We still keep videos paused until markerFound
        // But we can pre-warm play() in muted mode to satisfy autoplay policies
        videoColor.muted = true;
        videoAlpha.muted = true;
        videoColor.play().catch(()=>{});
        videoAlpha.play().catch(()=>{});
        videoColor.pause();
        videoAlpha.pause();
      }

      tapBtn.addEventListener('click', unlockAudio);

      // Marker events from AR.js
      marker.addEventListener('markerFound', async () => {
        videoPlane.setAttribute('visible', true);

        // Unmute color video if user granted gesture
        if (audioUnlocked) {
          videoColor.muted = false;
          videoColor.volume = 0.8;
        } else {
          videoColor.muted = true;
        }

        // Start both videos in sync
        try {
          await Promise.all([videoAlpha.play(), videoColor.play()]);
        } catch (e) {
          // Some devices need another user gesture; fail silently
          console.warn('Play blocked, waiting for user gesture', e);
        }
      });

      marker.addEventListener('markerLost', () => {
        // Hide and pause when the marker isn't visible
        videoPlane.setAttribute('visible', false);
        videoColor.pause();
        videoAlpha.pause();
      });

      // Safety: also pause videos when page visibility changes
      document.addEventListener('visibilitychange', () => {
        if (document.hidden) {
          videoColor.pause();
          videoAlpha.pause();
        }
      });
    });
  </script>
</body>
</html>
