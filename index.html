<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Back Home AR — WebXR Hit-Test (Camera On)</title>

  <!-- A-Frame -->
  <script src="https://aframe.io/releases/1.4.0/aframe.min.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover" />

  <style>
    html, body {
      margin: 0; height: 100%; overflow: hidden; background: transparent;
      font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif;
    }
    /* Ensure WebGL canvas is transparent so the AR camera shows behind it */
    canvas.a-canvas { background: transparent !important; }

    #tapOverlay {
      position: fixed; inset: 0; display: grid; place-items: center;
      background: rgba(0,0,0,0.7); color: #fff; z-index: 10; text-align: center;
    }
    #tapOverlay button {
      pointer-events: auto;
      background: #1976d2; color: #fff; border: 0; border-radius: 10px;
      padding: 14px 20px; font-weight: 700; font-size: 16px;
      box-shadow: 0 6px 16px rgba(0,0,0,.25); cursor: pointer;
    }
    #tip { margin-top: 10px; font-size: 14px; opacity: .9; }
    .hidden { display: none !important; }
  </style>

  <!-- Fragment shader for video + alpha compositing -->
  <script id="matte-shader" type="x-shader/x-fragment">
    precision mediump float;
    uniform sampler2D videoColor;
    uniform sampler2D videoAlpha;
    uniform float opacity;
    varying vec2 vUV;
    void main() {
      vec4 color = texture2D(videoColor, vUV);
      float alpha = texture2D(videoAlpha, vUV).r * opacity;
      gl_FragColor = vec4(color.rgb, alpha);
    }
  </script>
</head>

<body>
  <!-- Simple overlay to get the required user gesture (triggers camera permission on iPhone) -->
  <div id="tapOverlay">
    <div>
      <button id="startBtn">Start AR</button>
      <div id="tip">Move your phone to find a surface, then tap to place.</div>
    </div>
  </div>

  <a-scene
    embedded
    vr-mode-ui="enabled: false"
    renderer="alpha: true; antialias: true; precision: medium"
    webxr="mode: ar; requiredFeatures: hit-test; optionalFeatures: dom-overlay; overlayElement: #tapOverlay"
  >
    <a-assets>
      <video id="videoColor"
             src="https://raw.githack.com/alevalve/AR_Crab_Backhome/main/AR_Video02_Color.mp4"
             preload="auto" loop muted crossorigin="anonymous" playsinline webkit-playsinline></video>
      <video id="videoAlpha"
             src="https://raw.githack.com/alevalve/AR_Crab_Backhome/main/AR_Video02_Alpha.mp4"
             preload="auto" loop muted crossorigin="anonymous" playsinline webkit-playsinline></video>
    </a-assets>

    <!-- Reticle follows current hit test -->
    <a-entity id="reticle" visible="false">
      <a-ring radius-inner="0.045" radius-outer="0.055" rotation="-90 0 0" material="side: double"></a-ring>
      <a-entity position="0 0 0">
        <a-triangle scale="0.06 0.06 0.06" position="0.09 0 0" rotation="-90 0 0"></a-triangle>
      </a-entity>
    </a-entity>

    <!-- Video plane (shown only after placement) -->
    <a-plane
      id="videoPlane"
      visible="false"
      width="1.2"
      height="0.67"
      material="shader: video-matte; videoColor: #videoColor; videoAlpha: #videoAlpha; transparent: true; alphaTest: 0.001; opacity: 1"
    ></a-plane>

    <a-camera wasd-controls="enabled: false"></a-camera>
  </a-scene>

  <script>
    // Register the matte shader
    AFRAME.registerShader('video-matte', {
      schema: {
        videoColor: { type: 'map', is: 'uniform' },
        videoAlpha: { type: 'map', is: 'uniform' },
        opacity:    { type: 'number', default: 1.0, is: 'uniform' }
      },
      fragmentShader: document.getElementById('matte-shader').textContent,
      vertexShader: `
        varying vec2 vUV;
        void main() {
          vUV = uv;
          gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);
        }
      `
    });

    (function () {
      const scene      = document.querySelector('a-scene');
      const startBtn   = document.getElementById('startBtn');
      const tapOverlay = document.getElementById('tapOverlay');

      const reticleEl  = document.getElementById('reticle');
      const videoPlane = document.getElementById('videoPlane');

      const videoColor = document.getElementById('videoColor');
      const videoAlpha = document.getElementById('videoAlpha');

      let audioUnlocked = false;
      let placed = false;
      let hitTestSource = null;
      let refSpace = null;

      function ensureTransparentClear() {
        // Transparent clear so real camera shows behind WebGL
        if (scene.renderer) scene.renderer.setClearColor(0x000000, 0);
      }

      // Enforce transparent clear once scene loads
      if (scene.hasLoaded) ensureTransparentClear();
      else scene.addEventListener('loaded', ensureTransparentClear);

      // Start AR on user gesture — this is what triggers the iPhone camera permission prompt.
      startBtn.addEventListener('click', async () => {
        audioUnlocked = true;

        // Warm up autoplay policy (muted), then pause until placed
        try {
          videoColor.muted = true; videoAlpha.muted = true;
          await Promise.allSettled([videoColor.play(), videoAlpha.play()]);
          videoColor.pause(); videoAlpha.pause();
        } catch (e) {}

        // Check if immersive-AR is supported, then enter AR (prompts camera on iOS)
        try {
          if (navigator.xr && await navigator.xr.isSessionSupported('immersive-ar')) {
            tapOverlay.classList.add('hidden');
            scene.enterVR(); // A-Frame requests an immersive-ar session under the hood
          } else {
            alert('WebXR immersive-AR not supported on this browser/device.');
          }
        } catch (e) {
          console.warn('WebXR support check failed:', e);
          alert('Unable to start AR session on this browser/device.');
        }
      });

      // WebXR session lifecycle
      scene.renderer.xr.addEventListener('sessionstart', async () => {
        ensureTransparentClear();

        const session = scene.renderer.xr.getSession();
        // Required reference spaces
        refSpace = await session.requestReferenceSpace('local');
        const viewerSpace = await session.requestReferenceSpace('viewer');

        // Hit test source
        hitTestSource = await session.requestHitTestSource({ space: viewerSpace });

        // Show reticle while searching for surfaces
        reticleEl.setAttribute('visible', true);
      });

      scene.renderer.xr.addEventListener('sessionend', () => {
        hitTestSource = null;
        refSpace = null;
        placed = false;
        reticleEl.setAttribute('visible', false);
        videoPlane.setAttribute('visible', false);
        videoColor.pause(); videoAlpha.pause();
      });

      // Per-frame: update reticle from hit test
      scene.renderer.setAnimationLoop((time, frame) => {
        if (!frame || !hitTestSource || !refSpace) return;

        if (placed) {
          reticleEl.setAttribute('visible', false);
          return;
        }

        const results = frame.getHitTestResults(hitTestSource);
        if (results.length > 0) {
          const hit = results[0];
          const pose = hit.getPose(refSpace);
          if (pose) {
            const { x, y, z } = pose.transform.position;
            const q = pose.transform.orientation;

            reticleEl.object3D.position.set(x, y, z);
            reticleEl.object3D.quaternion.set(q.x, q.y, q.z, q.w);
            reticleEl.setAttribute('visible', true);
          }
        } else {
          reticleEl.setAttribute('visible', false);
        }
      });

      // Tap anywhere to place on the current reticle
      window.addEventListener('click', async () => {
        const inXR = scene.renderer.xr.isPresenting;
        if (!inXR || placed || reticleEl.getAttribute('visible') === false) return;

        placed = true;

        const pos = reticleEl.object3D.position.clone();
        const rot = reticleEl.object3D.rotation.clone();

        videoPlane.object3D.position.copy(pos);
        // Align plane upright on the surface; keep yaw from reticle
        videoPlane.object3D.rotation.set(-Math.PI / 2, rot.y, 0);

        videoPlane.setAttribute('visible', true);

        // Unmute if user tapped Start
        if (audioUnlocked) {
          videoColor.muted = false;
          videoColor.volume = 0.8;
        } else {
          videoColor.muted = true;
        }

        try {
          await Promise.all([videoAlpha.play(), videoColor.play()]);
        } catch (e) {
          console.warn('Playback blocked; will try again on next tap:', e);
        }
      });
    })();
  </script>
</body>
</html>
